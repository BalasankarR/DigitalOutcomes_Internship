{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Company Data from Fortune (We need to know the name of the companies to be able to do the rest of the scraping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r'C:/Users/DELL-PC/OneDrive/Desktop/chromedriver.exe')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('fortune2021.csv', 'w', encoding='utf-8', newline='')\n",
    "writer = csv.writer(csv_file)\n",
    "writer.writerow(['rank','company','revenues','revenue % change', 'profits','profits % change','assets','market val','change in rank 1000','employees','change in rank 500'])\n",
    "\n",
    "years_list = [2021]\n",
    "s = \"https://fortune.com/fortune500/{}/search/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://fortune.com/fortune500/2021/search/\n",
      "Scraping Page number 1\n",
      "Scraping Page number 2\n",
      "Scraping Page number 3\n",
      "Scraping Page number 4\n",
      "Scraping Page number 5\n",
      "Scraping Page number 6\n",
      "Scraping Page number 7\n",
      "Scraping Page number 8\n",
      "Scraping Page number 9\n",
      "Scraping Page number 10\n",
      "Scraping Page number 11\n",
      "Scraping Page number 12\n",
      "Scraping Page number 13\n",
      "Scraping Page number 14\n",
      "Scraping Page number 15\n",
      "Scraping Page number 16\n",
      "Scraping Page number 17\n",
      "Scraping Page number 18\n",
      "Scraping Page number 19\n",
      "Scraping Page number 20\n",
      "Scraping Page number 21\n",
      "Scraping Page number 22\n",
      "Scraping Page number 23\n",
      "Scraping Page number 24\n",
      "Scraping Page number 25\n",
      "Scraping Page number 26\n",
      "Scraping Page number 27\n",
      "Scraping Page number 28\n",
      "Scraping Page number 29\n",
      "Scraping Page number 30\n",
      "Scraping Page number 31\n",
      "Scraping Page number 32\n",
      "Scraping Page number 33\n",
      "Scraping Page number 34\n",
      "Scraping Page number 35\n",
      "Scraping Page number 36\n",
      "Scraping Page number 37\n",
      "Scraping Page number 38\n",
      "Scraping Page number 39\n",
      "Scraping Page number 40\n",
      "Scraping Page number 41\n",
      "Scraping Page number 42\n",
      "Scraping Page number 43\n",
      "Scraping Page number 44\n",
      "Scraping Page number 45\n",
      "Scraping Page number 46\n",
      "Scraping Page number 47\n",
      "Scraping Page number 48\n",
      "Scraping Page number 49\n",
      "Scraping Page number 50\n"
     ]
    }
   ],
   "source": [
    "for i in years_list:\n",
    "    url = s.format(i)\n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Page index used to keep track of where we are.\n",
    "    index = 1\n",
    "    while True:\n",
    "        if index > 50:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            print(\"Scraping Page number \" + str(index))\n",
    "            index = index + 1\n",
    "            # Find all the rows on the page\n",
    "            wait_row = WebDriverWait(driver, 30)\n",
    "            rows = wait_row.until(EC.presence_of_all_elements_located((By.XPATH,\n",
    "                                        '//div[@class=\"rt-tr-group\"]')))\n",
    "            for row in rows:\n",
    "                # Initialize an empty dictionary for each review\n",
    "                row_dict = {}\n",
    "                # Use relative xpath to locate the title, text, username, date, rating.\n",
    "                # Once you locate the element, you can use 'element.text' to return its string.\n",
    "                # To get the attribute instead of the text of each element, use 'element.get_attribute()'\n",
    "                try:\n",
    "                    rank = row.find_element_by_xpath('.//div[@class=\"rt-td searchResults__cell--2Y7Ce searchResults__rank--1sTfo\"]//span').text\n",
    "                    row_dict['rank'] = rank\n",
    "                except:\n",
    "                    rank = None\n",
    "                try:\n",
    "                    company = row.find_element_by_xpath('.//div[@class=\"rt-td searchResults__cell--2Y7Ce searchResults__title--3LyRA\"]//span/div').text\n",
    "                    row_dict['company'] = company\n",
    "                except:\n",
    "                    company = None\n",
    "\n",
    "                try:\n",
    "                    other_vals = row.find_elements_by_xpath('.//div[@class=\"rt-td searchResults__cell--2Y7Ce\"]//span') \n",
    "                    other_vals = [val.text for val in other_vals]  \n",
    "                    row_dict['revenues'] = other_vals[0]\n",
    "                    row_dict['revenue % change'] = other_vals[1]\n",
    "                    row_dict['profits'] = other_vals[2]\n",
    "                    row_dict['profits % change'] = other_vals[3]\n",
    "                    row_dict['assets']= other_vals[4]\n",
    "                    row_dict['market value'] = other_vals[5]\n",
    "                    row_dict['change in rank 1000'] = other_vals[6]\n",
    "                    row_dict['employees'] = other_vals[7]\n",
    "                    row_dict['change in rank 500'] = other_vals[8]\n",
    "                except:\n",
    "                    row_dict['revenues'] = None\n",
    "                    row_dict['revenue % change'] = None\n",
    "                    row_dict['profits'] = None\n",
    "                    row_dict['profits % change'] = None\n",
    "                    row_dict['assets']= None\n",
    "                    row_dict['market value'] = None\n",
    "                    row_dict['change in rank 1000'] = None\n",
    "                    row_dict['employees'] = None\n",
    "                    row_dict['change in rank 500'] = None\n",
    "\n",
    "\n",
    "                writer.writerow(row_dict.values())\n",
    "\n",
    "            # Locate the next button on the page.\n",
    "            time.sleep(3)\n",
    "            next_button = driver.find_element_by_xpath('//div[@class=\"-next\"]')\n",
    "            next_button.click()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "csv_file.close()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ForbesAPI to access the Forbes Global 2000 list and then getting the data in the form we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"accept\": \"application/json, text/plain, */*\",\n",
    "    \"referer\": \"https://www.forbes.com/global2000/\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.67 Safari/537.36\",\n",
    "}\n",
    "\n",
    "cookies = {\n",
    "    \"notice_behavior\": \"expressed,eu\",\n",
    "    \"notice_gdpr_prefs\": \"0,1,2:1a8b5228dd7ff0717196863a5d28ce6c\",\n",
    "}\n",
    "\n",
    "api_url = \"https://www.forbes.com/forbesapi/org/global2000/2021/position/true.json?limit=2000\"\n",
    "response = requests.get(api_url, headers=headers, cookies=cookies).json()\n",
    "\n",
    "sorted_list =  sorted(response[\"organizationList\"][\"organizationsLists\"], key=lambda k: k[\"position\"])\n",
    "df = pd.DataFrame(sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['name','year','month','description','listUri','organization','visible','timestamp','version','imageExists','image'],inplace = True)\n",
    "df.drop(columns=['revenueList','assetsList','profitList','employeesList','date','csfDisplayFields','industryLeader','employees','thumbnail'],inplace = True)\n",
    "df.drop(columns=['squareImage', 'portraitImage','landscapeImage', 'premiumProfile', 'clients', 'ceoCompensations','naturalId'],inplace = True)\n",
    "df.drop(columns = ['profits','assets','marketValue','profitsRank','assetsRank','marketValueRank','ceoTitle','yearFounded'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('revenue',ascending = False,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.country != 'United States'].index,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"organizationName\": \"Company\", \"industry\": \"Industry\", \"revenue\" : \"Revenue\", \"webSite\" : \"Website\", \"ceoName\" : \"CEO\"},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['uri','rank','position','country','revenueRank'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Headquarters'] = (df['city'] + ', ' + df['state'])\n",
    "df.drop(columns = ['city','state'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>CEO</th>\n",
       "      <th>Website</th>\n",
       "      <th>Headquarters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Walmart</td>\n",
       "      <td>Retailing</td>\n",
       "      <td>559151.0</td>\n",
       "      <td>C. Douglas McMillon</td>\n",
       "      <td>http://www.walmart.com</td>\n",
       "      <td>Bentonville, Arkansas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Amazon</td>\n",
       "      <td>Retailing</td>\n",
       "      <td>386064.0</td>\n",
       "      <td>Jeffrey P. Bezos</td>\n",
       "      <td>http://www.amazon.com</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Technology Hardware &amp; Equipment</td>\n",
       "      <td>293971.0</td>\n",
       "      <td>Tim Cook</td>\n",
       "      <td>http://www.apple.com</td>\n",
       "      <td>Cupertino, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CVS Health</td>\n",
       "      <td>Retailing</td>\n",
       "      <td>268654.0</td>\n",
       "      <td>Karen S. Lynch</td>\n",
       "      <td>http://cvshealth.com</td>\n",
       "      <td>Woonsocket, Rhode Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>Insurance</td>\n",
       "      <td>262916.0</td>\n",
       "      <td>Andrew Philip Witty</td>\n",
       "      <td>http://www.unitedhealthgroup.com</td>\n",
       "      <td>Minnetonka, Minnesota</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Company                         Industry   Revenue  \\\n",
       "17             Walmart                        Retailing  559151.0   \n",
       "9               Amazon                        Retailing  386064.0   \n",
       "5                Apple  Technology Hardware & Equipment  293971.0   \n",
       "36          CVS Health                        Retailing  268654.0   \n",
       "20  UnitedHealth Group                        Insurance  262916.0   \n",
       "\n",
       "                    CEO                           Website  \\\n",
       "17  C. Douglas McMillon            http://www.walmart.com   \n",
       "9      Jeffrey P. Bezos             http://www.amazon.com   \n",
       "5              Tim Cook              http://www.apple.com   \n",
       "36       Karen S. Lynch              http://cvshealth.com   \n",
       "20  Andrew Philip Witty  http://www.unitedhealthgroup.com   \n",
       "\n",
       "                Headquarters  \n",
       "17     Bentonville, Arkansas  \n",
       "9        Seattle, Washington  \n",
       "5      Cupertino, California  \n",
       "36  Woonsocket, Rhode Island  \n",
       "20     Minnetonka, Minnesota  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fortune Company Data is in the file fortune2021.csv, we load that data and we use the forbes data that we have to find the Companies for which the data we need is already in the Forbes data. The Missing Company data will be obtained by scraping their Wikipedia Page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"fortune2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "forbes_comp_list = df['Company'].to_list()\n",
    "fortune_comp_list = df1['company'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fortune_comp_list)):\n",
    "    fortune_comp_list[i] = ''.join(e for e in fortune_comp_list[i] if e.isalnum())\n",
    "for i in range(len(forbes_comp_list)):\n",
    "    forbes_comp_list[i] = ''.join(e for e in forbes_comp_list[i] if e.isalnum())\n",
    "forbes_comp_list = [string.replace(\"UnitedParcelService\",\"UPS\") for string in forbes_comp_list]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Name'] = forbes_comp_list\n",
    "df1['Name'] = fortune_comp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the companies that are missing so that we can look at their Wikipedia Pages for the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_companies = []\n",
    "for i in range(len(fortune_comp_list)):\n",
    "    temp = fortune_comp_list[i]\n",
    "    flag = 0\n",
    "    for j in range(len(forbes_comp_list)):\n",
    "        if(forbes_comp_list[j].find(temp) != -1 or temp.find(forbes_comp_list[j]) != -1):\n",
    "            flag = 1\n",
    "            continue\n",
    "    if(flag == 1):\n",
    "        continue\n",
    "    missing_companies.append((temp,i+1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('StateFarmInsurance', 39), ('NewYorkLifeInsurance', 67), ('PublixSuperMarkets', 69), ('LibertyMutualInsuranceGroup', 71), ('AIG', 72), ('Nationwide', 76), ('TIAA', 79), ('EnergyTransfer', 81), ('USAA', 87), ('NorthwesternMutual', 90), ('CHS', 103), ('EnterpriseProductsPartners', 105), ('Synnex', 117), ('MassachusettsMutualLifeInsurance', 123), ('PlainsGPHoldings', 127), ('NGLEnergyPartners', 151), ('Nvidia', 184), ('LaboratoryCorpofAmerica', 218), ('LandOLakes', 219), ('GuardianLifeInsCoofAmerica', 227), ('AmericanFamilyInsuranceGroup', 232), ('PeterKiewitSons', 243), ('WESCOInternational', 245), ('FarmersInsuranceExchange', 256), ('HollyFrontier', 279), ('BedBathBeyond', 280), ('MutualofOmahaInsurance', 282), ('Group1Automotive', 286), ('Nordstrom', 289), ('JonesFinancialEdwardJones', 295), ('ExpeditorsIntlofWashington', 299), ('PacificLife', 303), ('SonicAutomotive', 308), ('UnitedStatesSteel', 310), ('ODP', 312), ('MolsonCoorsBeverage', 314), ('KKR', 316), ('LibertyMedia', 326), ('SpartanNash', 329), ('Alcoa', 330), ('AutoOwnersInsurance', 341), ('EMCORGroup', 344), ('OwensMinor', 345), ('ErieInsuranceGroup', 347), ('CommScopeHolding', 356), ('RyderSystem', 357), ('FifthThirdBancorp', 358), ('InsightEnterprises', 360), ('GlobalPartners', 361), ('UnivarSolutions', 362), ('YumChinaHoldings', 363), ('TargaResources', 364), ('Andersons', 366), ('ThriventFinancialforLutherans', 369), ('CaseysGeneralStores', 371), ('WesternSouthernFinancialGroup', 374), ('CitizensFinancialGroup', 381), ('FootLocker', 385), ('NavistarInternational', 389), ('MagellanHealth', 390), ('Autoliv', 392), ('DelekUSHoldings', 397), ('GraybarElectric', 399), ('FrontierCommunications', 402), ('PVH', 404), ('AsburyAutomotiveGroup', 405), ('Seaboard', 406), ('Polaris', 407), ('ScienceApplicationsInternational', 412), ('XeroxHoldings', 415), ('Sanmina', 418), ('BeaconRoofingSupply', 420), ('SecurianFinancialGroup', 421), ('Oshkosh', 422), ('FMGlobal', 423), ('Coty', 426), ('Hanesbrands', 432), ('GraphicPackagingHolding', 435), ('SproutsFarmersMarket', 437), ('Veritiv', 439), ('MasTec', 441), ('DCPMidstream', 442), ('RealogyHoldings', 445), ('NCR', 446), ('RalphLauren', 450), ('TaylorMorrisonHome', 452), ('IcahnEnterprises', 453), ('BlackstoneGroup', 454), ('OIGlass', 455), ('NOV', 457), ('Ovintiv', 458), ('Huntsman', 460), ('ABMIndustries', 462), ('Ingredion', 463), ('SinclairBroadcastGroup', 465), ('KBR', 470), ('Olin', 472), ('CACIInternational', 473), ('PostHoldings', 474), ('AcademySportsandOutdoors', 475), ('Arconic', 476), ('Nasdaq', 480), ('MDUResourcesGroup', 486), ('SelectMedicalHoldings', 487), ('Patterson', 491), ('CommercialMetals', 492), ('BoiseCascade', 493), ('Hasbro', 494), ('AMarkPreciousMetals', 495), ('CampingWorldHoldings', 496), ('AvisBudgetGroup', 498), ('RRDonnelleySons', 499)]\n"
     ]
    }
   ],
   "source": [
    "print(missing_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_list = df1['company'].tolist()\n",
    "comp = []\n",
    "for string in companies_list:\n",
    "    new_s = string.replace(\" \",\"_\");\n",
    "    comp.append(new_s)\n",
    "url = \"https://en.wikipedia.org/wiki/{}\"\n",
    "missing_ranks = []\n",
    "for i in missing_companies:\n",
    "    missing_ranks.append(i[1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.Name.isin(df1.Name)]\n",
    "df2.to_csv(\"Company_Details.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "csv_file = open('Company_Details.csv', 'a', encoding='utf-8', newline='')\n",
    "writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_list = df1['company'].tolist()\n",
    "r_list = df1['revenues'].tolist()\n",
    "revenues_list = []\n",
    "for i in r_list:\n",
    "    temp = i.replace('$','')\n",
    "    revenues_list.append(temp.replace(',',''))\n",
    "revenues_list = list(map(float,revenues_list))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from here I am looking at their Wikipedia pages, there are a few rounds as not all companies have pages as https://en.wikipedia.org/wiki/{Name}. I will first try the name with underscore (i.e _), then with spaces, and then finally if both the above methods fail, i will go the Search page in Wikipedia, use the search phrase as the company name and try to filter out the Company's Page from the different results that we get using keywords like America, Company, Corporation, Inc. and a few others. After each run, we again compute the missing companies so wee can search for those companies alone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/State_Farm_Insurance\n",
      "https://en.wikipedia.org/wiki/New_York_Life_Insurance\n",
      "https://en.wikipedia.org/wiki/Publix_Super_Markets\n",
      "https://en.wikipedia.org/wiki/Liberty_Mutual_Insurance_Group\n",
      "https://en.wikipedia.org/wiki/AIG\n",
      "https://en.wikipedia.org/wiki/TIAA\n",
      "https://en.wikipedia.org/wiki/USAA\n",
      "https://en.wikipedia.org/wiki/Northwestern_Mutual\n",
      "https://en.wikipedia.org/wiki/Enterprise_Products_Partners\n",
      "https://en.wikipedia.org/wiki/Synnex\n",
      "https://en.wikipedia.org/wiki/Massachusetts_Mutual_Life_Insurance\n",
      "https://en.wikipedia.org/wiki/Plains_GP_Holdings\n",
      "https://en.wikipedia.org/wiki/Nvidia\n",
      "https://en.wikipedia.org/wiki/Laboratory_Corp._of_America\n",
      "https://en.wikipedia.org/wiki/Land_O'Lakes\n",
      "https://en.wikipedia.org/wiki/American_Family_Insurance_Group\n",
      "https://en.wikipedia.org/wiki/Peter_Kiewit_Sons'\n",
      "https://en.wikipedia.org/wiki/WESCO_International\n",
      "https://en.wikipedia.org/wiki/Farmers_Insurance_Exchange\n",
      "https://en.wikipedia.org/wiki/HollyFrontier\n",
      "https://en.wikipedia.org/wiki/Bed_Bath_&_Beyond\n",
      "https://en.wikipedia.org/wiki/Mutual_of_Omaha_Insurance\n",
      "https://en.wikipedia.org/wiki/Group_1_Automotive\n",
      "https://en.wikipedia.org/wiki/Nordstrom\n",
      "https://en.wikipedia.org/wiki/Pacific_Life\n",
      "https://en.wikipedia.org/wiki/Sonic_Automotive\n",
      "https://en.wikipedia.org/wiki/United_States_Steel\n",
      "https://en.wikipedia.org/wiki/KKR\n",
      "https://en.wikipedia.org/wiki/Liberty_Media\n",
      "https://en.wikipedia.org/wiki/SpartanNash\n",
      "https://en.wikipedia.org/wiki/Alcoa\n",
      "https://en.wikipedia.org/wiki/Auto-Owners_Insurance\n",
      "https://en.wikipedia.org/wiki/EMCOR_Group\n",
      "https://en.wikipedia.org/wiki/Owens_&_Minor\n",
      "https://en.wikipedia.org/wiki/Erie_Insurance_Group\n",
      "https://en.wikipedia.org/wiki/Ryder_System\n",
      "https://en.wikipedia.org/wiki/Fifth_Third_Bancorp\n",
      "https://en.wikipedia.org/wiki/Insight_Enterprises\n",
      "https://en.wikipedia.org/wiki/Global_Partners\n",
      "https://en.wikipedia.org/wiki/Univar_Solutions\n",
      "https://en.wikipedia.org/wiki/Yum_China_Holdings\n",
      "https://en.wikipedia.org/wiki/Targa_Resources\n",
      "https://en.wikipedia.org/wiki/Thrivent_Financial_for_Lutherans\n",
      "https://en.wikipedia.org/wiki/Casey's_General_Stores\n",
      "https://en.wikipedia.org/wiki/Western_&_Southern_Financial_Group\n",
      "https://en.wikipedia.org/wiki/Citizens_Financial_Group\n",
      "https://en.wikipedia.org/wiki/Foot_Locker\n",
      "https://en.wikipedia.org/wiki/Navistar_International\n",
      "https://en.wikipedia.org/wiki/Magellan_Health\n",
      "https://en.wikipedia.org/wiki/Autoliv\n",
      "https://en.wikipedia.org/wiki/Delek_US_Holdings\n",
      "https://en.wikipedia.org/wiki/Graybar_Electric\n",
      "https://en.wikipedia.org/wiki/Frontier_Communications\n",
      "https://en.wikipedia.org/wiki/Asbury_Automotive_Group\n",
      "https://en.wikipedia.org/wiki/Science_Applications_International\n",
      "https://en.wikipedia.org/wiki/Xerox_Holdings\n",
      "https://en.wikipedia.org/wiki/Sanmina\n",
      "https://en.wikipedia.org/wiki/Beacon_Roofing_Supply\n",
      "https://en.wikipedia.org/wiki/Securian_Financial_Group\n",
      "https://en.wikipedia.org/wiki/Hanesbrands\n",
      "https://en.wikipedia.org/wiki/Sprouts_Farmers_Market\n",
      "https://en.wikipedia.org/wiki/Veritiv\n",
      "https://en.wikipedia.org/wiki/MasTec\n",
      "https://en.wikipedia.org/wiki/DCP_Midstream\n",
      "https://en.wikipedia.org/wiki/Ralph_Lauren\n",
      "https://en.wikipedia.org/wiki/Icahn_Enterprises\n",
      "https://en.wikipedia.org/wiki/Blackstone_Group\n",
      "https://en.wikipedia.org/wiki/O-I_Glass\n",
      "https://en.wikipedia.org/wiki/Ovintiv\n",
      "https://en.wikipedia.org/wiki/ABM_Industries\n",
      "https://en.wikipedia.org/wiki/Ingredion\n",
      "https://en.wikipedia.org/wiki/Sinclair_Broadcast_Group\n",
      "https://en.wikipedia.org/wiki/CACI_International\n",
      "https://en.wikipedia.org/wiki/Post_Holdings\n",
      "https://en.wikipedia.org/wiki/Academy_Sports_and_Outdoors\n",
      "https://en.wikipedia.org/wiki/Arconic\n",
      "https://en.wikipedia.org/wiki/Nasdaq\n",
      "https://en.wikipedia.org/wiki/Commercial_Metals\n",
      "https://en.wikipedia.org/wiki/Boise_Cascade\n",
      "https://en.wikipedia.org/wiki/Hasbro\n",
      "https://en.wikipedia.org/wiki/A-Mark_Precious_Metals\n",
      "https://en.wikipedia.org/wiki/Avis_Budget_Group\n",
      "https://en.wikipedia.org/wiki/R.R._Donnelley_&_Sons\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/{}\"\n",
    "\n",
    "for i in missing_ranks:\n",
    "    count = i-1\n",
    "    comp_url = url.format(comp[i-1]) \n",
    "    \n",
    "    row_dict = {}\n",
    "\n",
    "    r = requests.get(html.unescape(comp_url).replace(\" \",\"\"))  \n",
    "    soup = BeautifulSoup(r.content, 'html5lib') \n",
    "    \n",
    "    noArticle = soup.findAll(\"div\", attrs = {\"class\": \"noarticletext mw-content-ltr\"})\n",
    "    multiplePages = soup.findAll(\"div\", attrs = {\"class\": \"tocright\"})\n",
    "    if(len(noArticle) != 0 or len(multiplePages) != 0):\n",
    "        continue \n",
    "   \n",
    "    label = soup.findAll(\"th\", attrs = {\"class\": \"infobox-label\"})\n",
    "    if(len(label) == 0):\n",
    "        continue\n",
    "    \n",
    "    print(comp_url)\n",
    "    label_list = []\n",
    "    for j in range(len(label)):\n",
    "        label_list.append(label[j].text)\n",
    "    \n",
    "    table = soup.findAll(\"td\", attrs = {\"class\": \"infobox-data\"})\n",
    "    table_list = []\n",
    "    for j in range(len(table)):\n",
    "        table_list.append(table[j].text)\n",
    "        \n",
    "    #Company\n",
    "    row_dict['Company'] = companies_list[count]     \n",
    "        \n",
    "    #Industry\n",
    "    flag = 0\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Industry\" or label_list[j] == \"Products\"):\n",
    "            flag = j\n",
    "            break\n",
    "\n",
    "    ind = table[j].find_all(\"a\",attrs = {\"href\":re.compile(r'/wiki/')})    \n",
    "    ind_list = []\n",
    "    for j in range(len(ind)):\n",
    "        ind_list.append(ind[j].text)\n",
    "    \n",
    "    if(flag == 0):\n",
    "        row_dict['Industry'] = \"\"\n",
    "    elif(len(ind_list) == 0):\n",
    "        row_dict['Industry'] = table_list[flag]\n",
    "    else:\n",
    "        s = \"\"\n",
    "        j = 0\n",
    "        while(j < len(ind_list) - 1):\n",
    "            s = s + ind_list[j] + \", \"\n",
    "            j = j + 1\n",
    "        s = s + ind_list[j]\n",
    "        row_dict['Industry'] = s    \n",
    "    \n",
    "    #Revenue\n",
    "    row_dict['Revenue'] = revenues_list[count]\n",
    "    \n",
    "    #CEO\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Key people\"):\n",
    "            break\n",
    "    \n",
    "    s = \"\"\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Key people\"):\n",
    "            s = s + table_list[j]\n",
    "            break\n",
    "    lst = []\n",
    "    lst1 = []\n",
    "    if(s.find('(') != -1):\n",
    "        s1 = s.split(')')\n",
    "        for k in range(len(s1)):\n",
    "            lst.append(s1[k])\n",
    "        for k in lst:\n",
    "            s1 = k.split('(')\n",
    "            for j in range(len(s1)):\n",
    "                lst1.append(s1[j])\n",
    "    else:\n",
    "        flag = 0\n",
    "        for j in range(len(label)):\n",
    "            if(label_list[j] == \"Key people\"):\n",
    "                flag = 1\n",
    "                break\n",
    "        if(flag == 1):\n",
    "            key = table_list[j]        \n",
    "            lst1 = key.split(\",\")\n",
    "        \n",
    "    if(len(lst1) == 0):\n",
    "        row_dict['CEO'] = \"\" \n",
    "    elif(len(lst1) == 1):\n",
    "        lst1 = lst1[0].split(\" \")\n",
    "        for k in range(len(lst1)):\n",
    "            if(lst1[k].find('CEO') != -1):\n",
    "                break\n",
    "        j = k-1\n",
    "        while(1):\n",
    "            if(lst1[j].find(\"President\") == -1 and lst1[j].find(\"Chairman\") == -1 and lst1[j].find(\", \") == -1 and lst1[j].find(\"& \") == -1 and lst1[j].find(\"and\") == -1):\n",
    "                break\n",
    "            elif(j == 0):\n",
    "                break\n",
    "            else:    \n",
    "                j = j - 1;\n",
    "\n",
    "        s = \"\"\n",
    "        for k in range(j+1):\n",
    "            s = s + lst1[k] + \" \"    \n",
    "        row_dict['CEO'] = s\n",
    "    else:\n",
    "        for k in range(len(lst1)):\n",
    "            if(lst1[k].find('CEO') != -1):\n",
    "                break\n",
    "        j = k-1\n",
    "        while(1):\n",
    "            if(lst1[j].find(\"President\") == -1 and lst1[j].find(\"Chairman\") == -1 and lst1[j].find(\", \") == -1 and lst1[j].find(\"& \") == -1):\n",
    "                break\n",
    "            elif(j == 0):\n",
    "                break\n",
    "            else:    \n",
    "                j = j - 1;\n",
    "        row_dict['CEO'] = lst1[j] \n",
    "    \n",
    "    #Website\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Website\"):\n",
    "            break\n",
    "    s = table_list[j]\n",
    "    s = 'https://' + s\n",
    "    row_dict['Website'] = s   \n",
    "    \n",
    "    #Headquarters\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Headquarters\"):\n",
    "            break\n",
    "    row_dict['Headquarters'] = table_list[j]\n",
    "\n",
    "    writer.writerow(row_dict.values())\n",
    "\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Company_Details.csv\")\n",
    "df1 = pd.read_csv(\"fortune2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = df['Company'].to_list()\n",
    "fortune_comp_list = df1['company'].to_list()\n",
    "for i in range(len(fortune_comp_list)):\n",
    "    fortune_comp_list[i] = ''.join(e for e in fortune_comp_list[i] if e.isalnum())\n",
    "for i in range(len(comp_list)):\n",
    "    comp_list[i] = ''.join(e for e in comp_list[i] if e.isalnum())\n",
    "comp_list = [string.replace(\"UnitedParcelService\",\"UPS\") for string in comp_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_companies = []\n",
    "for i in range(len(fortune_comp_list)):\n",
    "    temp = fortune_comp_list[i]\n",
    "    flag = 0\n",
    "    for j in range(len(comp_list)):\n",
    "        if(comp_list[j].find(temp) != -1 or temp.find(comp_list[j]) != -1):\n",
    "            flag = 1\n",
    "            continue\n",
    "    if(flag == 1):\n",
    "        continue\n",
    "    missing_companies.append((temp,i+1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('WalgreensBootsAlliance', 16), ('Merck', 65), ('Nationwide', 76), ('EnergyTransfer', 81), ('Deere', 88), ('TJX', 97), ('CapitalOneFinancial', 99), ('CHS', 103), ('Jabil', 104), ('KraftHeinz', 110), ('PNCFinancialServicesGroup', 120), ('USFoodsHolding', 128), ('PayPalHoldings', 134), ('Salesforce', 137), ('BakerHughes', 140), ('HartfordFinancialServicesGroup', 142), ('PenskeAutomotiveGroup', 143), ('DuPont', 144), ('NGLEnergyPartners', 151), ('ManpowerGroup', 165), ('CognizantTechnologySolutions', 185), ('AECOM', 189), ('CHRobinsonWorldwide', 191), ('AmericanElectricPower', 204), ('PrincipalFinancial', 206), ('EstÃ©eLauder', 213), ('QurateRetail', 216), ('PPGIndustries', 220), ('JacobsEngineeringGroup', 225), ('GuardianLifeInsCoofAmerica', 227), ('UnumGroup', 230), ('FidelityNationalInformationServices', 241), ('GoodyearTireRubber', 246), ('LeidosHoldings', 248), ('Newmont', 273), ('Vistra', 274), ('IQVIAHoldings', 275), ('UberTechnologies', 281), ('JonesFinancialEdwardJones', 295), ('ExpeditorsIntlofWashington', 299), ('ODP', 312), ('MolsonCoorsBeverage', 314), ('RelianceSteelAluminum', 343), ('CommScopeHolding', 356), ('Andersons', 366), ('PVH', 404), ('Seaboard', 406), ('Polaris', 407), ('ZimmerBiometHoldings', 414), ('ArthurJGallagher', 416), ('JefferiesFinancialGroup', 419), ('Oshkosh', 422), ('FMGlobal', 423), ('Coty', 426), ('GraphicPackagingHolding', 435), ('RealogyHoldings', 445), ('NCR', 446), ('TaylorMorrisonHome', 452), ('NOV', 457), ('Huntsman', 460), ('KBR', 470), ('Olin', 472), ('MDUResourcesGroup', 486), ('SelectMedicalHoldings', 487), ('Patterson', 491), ('CampingWorldHoldings', 496)]\n"
     ]
    }
   ],
   "source": [
    "print(missing_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('Company_Details.csv', 'a', encoding='utf-8', newline='')\n",
    "writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_ranks = []\n",
    "missing_ranks_copy = []\n",
    "missing_companies_list = []\n",
    "for i in missing_companies:\n",
    "    missing_ranks.append(i[1])\n",
    "    missing_ranks_copy.append(i[1])\n",
    "    missing_companies_list.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Walgreens Boots Alliance\n",
      "https://en.wikipedia.org/wiki/TJX\n",
      "https://en.wikipedia.org/wiki/Capital One Financial\n",
      "https://en.wikipedia.org/wiki/Jabil\n",
      "https://en.wikipedia.org/wiki/Kraft Heinz\n",
      "https://en.wikipedia.org/wiki/PNC Financial Services Group\n",
      "https://en.wikipedia.org/wiki/Salesforce\n",
      "https://en.wikipedia.org/wiki/Baker Hughes\n",
      "https://en.wikipedia.org/wiki/Hartford Financial Services Group\n",
      "https://en.wikipedia.org/wiki/Penske Automotive Group\n",
      "https://en.wikipedia.org/wiki/DuPont\n",
      "https://en.wikipedia.org/wiki/ManpowerGroup\n",
      "https://en.wikipedia.org/wiki/Cognizant Technology Solutions\n",
      "https://en.wikipedia.org/wiki/AECOM\n",
      "https://en.wikipedia.org/wiki/C.H. Robinson Worldwide\n",
      "https://en.wikipedia.org/wiki/American Electric Power\n",
      "https://en.wikipedia.org/wiki/Principal Financial\n",
      "https://en.wikipedia.org/wiki/Qurate Retail\n",
      "https://en.wikipedia.org/wiki/PPG Industries\n",
      "https://en.wikipedia.org/wiki/Jacobs Engineering Group\n",
      "https://en.wikipedia.org/wiki/Unum Group\n",
      "https://en.wikipedia.org/wiki/Fidelity National Information Services\n",
      "https://en.wikipedia.org/wiki/Goodyear Tire & Rubber\n",
      "https://en.wikipedia.org/wiki/Leidos Holdings\n",
      "https://en.wikipedia.org/wiki/Newmont\n",
      "https://en.wikipedia.org/wiki/Vistra\n",
      "https://en.wikipedia.org/wiki/Uber Technologies\n",
      "https://en.wikipedia.org/wiki/Reliance Steel & Aluminum\n",
      "https://en.wikipedia.org/wiki/Arthur J. Gallagher\n",
      "https://en.wikipedia.org/wiki/Jefferies Financial Group\n"
     ]
    }
   ],
   "source": [
    "companies_list = df1['company'].tolist()\n",
    "r_list = df1['revenues'].tolist()\n",
    "revenues_list = []\n",
    "for i in r_list:\n",
    "    temp = i.replace('$','')\n",
    "    revenues_list.append(temp.replace(',',''))\n",
    "revenues_list = list(map(float,revenues_list)) \n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/{}\"\n",
    "\n",
    "for a in range(len(missing_ranks)):\n",
    "    i = missing_ranks[a]\n",
    "    count = i-1\n",
    "    comp_url = url.format(companies_list[i-1]) \n",
    "    \n",
    "    row_dict = {}\n",
    "\n",
    "    r = requests.get(html.unescape(comp_url).replace(\" \",\"_\"))  \n",
    "    soup = BeautifulSoup(r.content, 'html5lib') \n",
    "    \n",
    "    noArticle = soup.findAll(\"div\", attrs = {\"class\": \"noarticletext mw-content-ltr\"})\n",
    "    multiplePages = soup.findAll(\"div\", attrs = {\"class\": \"tocright\"})\n",
    "    if(len(noArticle) != 0 or len(multiplePages) != 0):\n",
    "        continue    \n",
    "    \n",
    "    label = soup.findAll(\"th\", attrs = {\"class\": \"infobox-label\"})\n",
    "    if(len(label) == 0):\n",
    "        continue\n",
    "    \n",
    "    print(comp_url)\n",
    "    label_list = []\n",
    "    for j in range(len(label)):\n",
    "        label_list.append(label[j].text)\n",
    "    \n",
    "    table = soup.findAll(\"td\", attrs = {\"class\": \"infobox-data\"})\n",
    "    table_list = []\n",
    "    for j in range(len(table)):\n",
    "        table_list.append(table[j].text)\n",
    "        \n",
    "    #Company\n",
    "    row_dict['Company'] = companies_list[count]     \n",
    "        \n",
    "    #Industry\n",
    "    flag = 0\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Industry\" or label_list[j] == \"Products\"):\n",
    "            flag = j\n",
    "            break\n",
    "\n",
    "    ind = table[j].find_all(\"a\",attrs = {\"href\":re.compile(r'/wiki/')})    \n",
    "    ind_list = []\n",
    "    for j in range(len(ind)):\n",
    "        ind_list.append(ind[j].text)\n",
    "    \n",
    "    if(flag == 0):\n",
    "        row_dict['Industry'] = \"\"\n",
    "    elif(len(ind_list) == 0):\n",
    "        row_dict['Industry'] = table_list[flag]\n",
    "    else:\n",
    "        s = \"\"\n",
    "        j = 0\n",
    "        while(j < len(ind_list) - 1):\n",
    "            s = s + ind_list[j] + \", \"\n",
    "            j = j + 1\n",
    "        s = s + ind_list[j]\n",
    "        row_dict['Industry'] = s    \n",
    "    \n",
    "    #Revenue\n",
    "    row_dict['Revenue'] = revenues_list[count]\n",
    "    \n",
    "    #CEO\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Key people\"):\n",
    "            break\n",
    "    \n",
    "    s = \"\"\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Key people\"):\n",
    "            s = s + table_list[j]\n",
    "            break\n",
    "    lst = []\n",
    "    lst1 = []\n",
    "    if(s.find('(') != -1):\n",
    "        s1 = s.split(')')\n",
    "        for k in range(len(s1)):\n",
    "            lst.append(s1[k])\n",
    "        for k in lst:\n",
    "            s1 = k.split('(')\n",
    "            for j in range(len(s1)):\n",
    "                lst1.append(s1[j])\n",
    "    else:\n",
    "        flag = 0\n",
    "        for j in range(len(label)):\n",
    "            if(label_list[j] == \"Key people\"):\n",
    "                flag = 1\n",
    "                break\n",
    "        if(flag == 1):\n",
    "            key = table_list[j]        \n",
    "            lst1 = key.split(\",\")\n",
    "        \n",
    "    if(len(lst1) == 0):\n",
    "        row_dict['CEO'] = \"\" \n",
    "    elif(len(lst1) == 1):\n",
    "        lst1 = lst1[0].split(\" \")\n",
    "        for k in range(len(lst1)):\n",
    "            if(lst1[k].find('CEO') != -1):\n",
    "                break\n",
    "        j = k-1\n",
    "        while(1):\n",
    "            if(lst1[j].find(\"President\") == -1 and lst1[j].find(\"Chairman\") == -1 and lst1[j].find(\", \") == -1 and lst1[j].find(\"& \") == -1 and lst1[j].find(\"and\") == -1):\n",
    "                break\n",
    "            elif(j == 0):\n",
    "                break\n",
    "            else:    \n",
    "                j = j - 1;\n",
    "\n",
    "        s = \"\"\n",
    "        for k in range(j+1):\n",
    "            s = s + lst1[k] + \" \"    \n",
    "        row_dict['CEO'] = s\n",
    "    else:\n",
    "        for k in range(len(lst1)):\n",
    "            if(lst1[k].find('CEO') != -1):\n",
    "                break\n",
    "        j = k-1\n",
    "        while(1):\n",
    "            if(lst1[j].find(\"President\") == -1 and lst1[j].find(\"Chairman\") == -1 and lst1[j].find(\", \") == -1 and lst1[j].find(\"& \") == -1):\n",
    "                break\n",
    "            elif(j == 0):\n",
    "                break\n",
    "            else:    \n",
    "                j = j - 1;\n",
    "        row_dict['CEO'] = lst1[j] \n",
    "    \n",
    "    #Website\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Website\"):\n",
    "            break\n",
    "    s = table_list[j]\n",
    "    s = 'https://' + s\n",
    "    row_dict['Website'] = s   \n",
    "    \n",
    "    #Headquarters\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Headquarters\"):\n",
    "            break\n",
    "    row_dict['Headquarters'] = table_list[j]\n",
    "\n",
    "    writer.writerow(row_dict.values())\n",
    "    missing_ranks_copy.remove(missing_ranks[a])\n",
    "    \n",
    "csv_file.close()\n",
    "missing_ranks = missing_ranks_copy\n",
    "#missing_ranks.remove(151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"fortune2021.csv\")\n",
    "companies = df1['company'].to_list()\n",
    "companies = [s.replace(\" \",\"+\") for s in companies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open('Company_Details.csv', 'a', encoding='utf-8', newline='')\n",
    "writer = csv.writer(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "\n",
    "URL = \"https://en.wikipedia.org/w/index.php?search={}&title=Special%3ASearch&fulltext=1&ns0=1\"\n",
    "for i in missing_ranks:\n",
    "    url = URL.format(companies[i-1])\n",
    "    r = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')\n",
    "    multiplePages = soup.findAll(\"div\", attrs = {\"class\": \"mw-search-result-heading\"})\n",
    "    \n",
    "    res = soup.findAll(\"div\", attrs = {\"class\": \"searchresult\"})\n",
    "    res_list = []\n",
    "    for i in range(len(res)):\n",
    "        res_list.append(res[i].text)\n",
    "    \n",
    "    k = 21\n",
    "    for j in range(len(res_list)):\n",
    "        a = res_list[j].lower()\n",
    "        lst = ['usa','america','corporation','inc.','company','firm','division']\n",
    "        for c in lst:\n",
    "            if(a.find(c) != -1):\n",
    "                k = j\n",
    "                break\n",
    "        if(k == j):\n",
    "            break\n",
    "    s = multiplePages[k].find_all(\"a\",attrs = {\"href\":re.compile(r'/wiki/')})\n",
    "    s1 = s[0]['href'].split('/')\n",
    "    l.append(s1[len(s1)-1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Merck_%26_Co.\n",
      "https://en.wikipedia.org/wiki/Energy_Transfer_Partners\n",
      "https://en.wikipedia.org/wiki/John_Deere\n",
      "https://en.wikipedia.org/wiki/CHS_Inc.\n",
      "https://en.wikipedia.org/wiki/Conagra_Brands\n",
      "https://en.wikipedia.org/wiki/PayPal\n",
      "https://en.wikipedia.org/wiki/The_Est%C3%A9e_Lauder_Companies\n",
      "https://en.wikipedia.org/wiki/The_Guardian_Life_Insurance_Company_of_America\n",
      "https://en.wikipedia.org/wiki/IQVIA\n",
      "https://en.wikipedia.org/wiki/Edward_Jones_Investments\n",
      "https://en.wikipedia.org/wiki/Expeditors_International\n",
      "https://en.wikipedia.org/wiki/The_ODP_Corporation\n",
      "https://en.wikipedia.org/wiki/Molson_Coors_Beverage_Company\n",
      "https://en.wikipedia.org/wiki/CommScope\n",
      "https://en.wikipedia.org/wiki/The_Andersons\n",
      "https://en.wikipedia.org/wiki/PVH_(company)\n",
      "https://en.wikipedia.org/wiki/Seaboard_Corporation\n",
      "https://en.wikipedia.org/wiki/Polaris_Inc.\n",
      "https://en.wikipedia.org/wiki/Zimmer_Biomet\n",
      "https://en.wikipedia.org/wiki/OshKosh_B%27gosh\n",
      "https://en.wikipedia.org/wiki/Coty_Inc.\n",
      "https://en.wikipedia.org/wiki/Realogy\n",
      "https://en.wikipedia.org/wiki/Taylor_Morrison\n",
      "https://en.wikipedia.org/wiki/NOV_Inc.\n",
      "https://en.wikipedia.org/wiki/Jon_Huntsman_Jr.\n",
      "https://en.wikipedia.org/wiki/KBR_(company)\n",
      "https://en.wikipedia.org/wiki/MDU_Resources\n",
      "https://en.wikipedia.org/wiki/Humana\n",
      "https://en.wikipedia.org/wiki/Floyd_Patterson\n",
      "https://en.wikipedia.org/wiki/Camping_World\n"
     ]
    }
   ],
   "source": [
    "companies_list = df1['company'].tolist()\n",
    "r_list = df1['revenues'].tolist()\n",
    "revenues_list = []\n",
    "for i in r_list:\n",
    "    temp = i.replace('$','')\n",
    "    revenues_list.append(temp.replace(',',''))\n",
    "revenues_list = list(map(float,revenues_list)) \n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/{}\"\n",
    "\n",
    "for a in range(len(missing_ranks)):\n",
    "    i = missing_ranks[a]-1\n",
    "    count = i\n",
    "    comp_url = url.format(l[a]) \n",
    "    \n",
    "    row_dict = {}\n",
    "\n",
    "    r = requests.get(comp_url)  \n",
    "    soup = BeautifulSoup(r.content, 'html5lib') \n",
    "    \n",
    "    noArticle = soup.findAll(\"div\", attrs = {\"class\": \"noarticletext mw-content-ltr\"})\n",
    "    multiplePages = soup.findAll(\"div\", attrs = {\"class\": \"tocright\"})\n",
    "    if(len(noArticle) != 0 or len(multiplePages) != 0):\n",
    "        continue    \n",
    "    \n",
    "    label = soup.findAll(\"th\", attrs = {\"class\": \"infobox-label\"})\n",
    "    if(len(label) == 0):\n",
    "        continue\n",
    "    \n",
    "    print(comp_url)\n",
    "    label_list = []\n",
    "    for j in range(len(label)):\n",
    "        label_list.append(label[j].text)\n",
    "    \n",
    "    table = soup.findAll(\"td\", attrs = {\"class\": \"infobox-data\"})\n",
    "    table_list = []\n",
    "    for j in range(len(table)):\n",
    "        table_list.append(table[j].text)\n",
    "        \n",
    "    #Company\n",
    "    row_dict['Company'] = companies_list[count]     \n",
    "        \n",
    "    #Industry\n",
    "    flag = 0\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Industry\" or label_list[j] == \"Products\"):\n",
    "            flag = j\n",
    "            break\n",
    "\n",
    "    ind = table[j].find_all(\"a\",attrs = {\"href\":re.compile(r'/wiki/')})    \n",
    "    ind_list = []\n",
    "    for j in range(len(ind)):\n",
    "        ind_list.append(ind[j].text)\n",
    "    \n",
    "    if(flag == 0):\n",
    "        row_dict['Industry'] = \"\"\n",
    "    elif(len(ind_list) == 0):\n",
    "        row_dict['Industry'] = table_list[flag]\n",
    "    else:\n",
    "        s = \"\"\n",
    "        j = 0\n",
    "        while(j < len(ind_list) - 1):\n",
    "            s = s + ind_list[j] + \", \"\n",
    "            j = j + 1\n",
    "        s = s + ind_list[j]\n",
    "        row_dict['Industry'] = s    \n",
    "    \n",
    "    #Revenue\n",
    "    row_dict['Revenue'] = revenues_list[count]\n",
    "    \n",
    "    #CEO\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Key people\"):\n",
    "            break\n",
    "    \n",
    "    s = \"\"\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Key people\"):\n",
    "            s = s + table_list[j]\n",
    "            break\n",
    "    lst = []\n",
    "    lst1 = []\n",
    "    if(s.find('(') != -1):\n",
    "        s1 = s.split(')')\n",
    "        for k in range(len(s1)):\n",
    "            lst.append(s1[k])\n",
    "        for k in lst:\n",
    "            s1 = k.split('(')\n",
    "            for j in range(len(s1)):\n",
    "                lst1.append(s1[j])\n",
    "    else:\n",
    "        flag = 0\n",
    "        for j in range(len(label)):\n",
    "            if(label_list[j] == \"Key people\"):\n",
    "                flag = 1\n",
    "                break\n",
    "        if(flag == 1):\n",
    "            key = table_list[j]        \n",
    "            lst1 = key.split(\",\")\n",
    "        \n",
    "    if(len(lst1) == 0):\n",
    "        row_dict['CEO'] = \"\" \n",
    "    elif(len(lst1) == 1):\n",
    "        lst1 = lst1[0].split(\" \")\n",
    "        for k in range(len(lst1)):\n",
    "            if(lst1[k].find('CEO') != -1):\n",
    "                break\n",
    "        j = k-1\n",
    "        while(1):\n",
    "            if(lst1[j].find(\"President\") == -1 and lst1[j].find(\"Chairman\") == -1 and lst1[j].find(\", \") == -1 and lst1[j].find(\"& \") == -1 and lst1[j].find(\"and\") == -1):\n",
    "                break\n",
    "            elif(j == 0):\n",
    "                break\n",
    "            else:    \n",
    "                j = j - 1;\n",
    "\n",
    "        s = \"\"\n",
    "        for k in range(j+1):\n",
    "            s = s + lst1[k] + \" \"    \n",
    "        row_dict['CEO'] = s\n",
    "    else:\n",
    "        for k in range(len(lst1)):\n",
    "            if(lst1[k].find('CEO') != -1):\n",
    "                break\n",
    "        j = k-1\n",
    "        while(1):\n",
    "            if(lst1[j].find(\"President\") == -1 and lst1[j].find(\"Chairman\") == -1 and lst1[j].find(\", \") == -1 and lst1[j].find(\"& \") == -1):\n",
    "                break\n",
    "            elif(j == 0):\n",
    "                break\n",
    "            else:    \n",
    "                j = j - 1;\n",
    "        row_dict['CEO'] = lst1[j] \n",
    "    \n",
    "    #Website\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Website\"):\n",
    "            break\n",
    "    s = table_list[j]\n",
    "    s = 'https://' + s\n",
    "    row_dict['Website'] = s   \n",
    "    \n",
    "    #Headquarters\n",
    "    for j in range(len(label)):\n",
    "        if(label_list[j] == \"Headquarters\"):\n",
    "            break\n",
    "    row_dict['Headquarters'] = table_list[j]\n",
    "\n",
    "    writer.writerow(row_dict.values())\n",
    "    \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Company_Details.csv\")\n",
    "df1 = pd.read_csv(\"fortune2021.csv\")\n",
    "forbes_comp_list = df['Company'].to_list()\n",
    "fortune_comp_list = df1['company'].to_list()\n",
    "for i in range(len(fortune_comp_list)):\n",
    "    fortune_comp_list[i] = ''.join(e for e in fortune_comp_list[i] if e.isalnum())\n",
    "for i in range(len(forbes_comp_list)):\n",
    "    forbes_comp_list[i] = ''.join(e for e in forbes_comp_list[i] if e.isalnum())\n",
    "forbes_comp_list = [string.replace(\"UnitedParcelService\",\"UPS\") for string in forbes_comp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Name'] = fortune_comp_list\n",
    "df['Name'] = forbes_comp_list\n",
    "Fortune_Rev = df1['revenues'].to_list()\n",
    "Fortune_Rank = df1['rank'].to_list()\n",
    "\n",
    "revenue_list = []\n",
    "rank_list = []\n",
    "for i in forbes_comp_list:\n",
    "    for j in range(len(fortune_comp_list)):\n",
    "        if(fortune_comp_list[j] == i):\n",
    "            revenue_list.append(Fortune_Rev[j])\n",
    "            rank_list.append(Fortune_Rank[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rev'] = revenue_list\n",
    "df['rank'] = rank_list\n",
    "df.sort_values('rank',inplace = True)\n",
    "df.drop(columns = ['Revenue','Name'],inplace = True)\n",
    "df.rename(columns={\"rev\": \"Revenue (in Millions)\", \"rank\": \"Rank\"},inplace = True)\n",
    "df = df[['Rank','Company','Revenue (in Millions)','Industry','CEO','Website','Headquarters']]\n",
    "df.to_csv(\"Company_Details.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
